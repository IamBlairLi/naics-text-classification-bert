model_name: distilbert-base-uncased
max_length: 128

train_file: data/train.jsonl
val_file: data/val.jsonl

output_dir: artifacts/distilbert_naics
num_train_epochs: 5
per_device_train_batch_size: 8
per_device_eval_batch_size: 8
learning_rate: 5.0e-5
weight_decay: 0.01
warmup_ratio: 0.1
seed: 42

logging_steps: 10
eval_strategy: epoch
save_strategy: epoch
save_total_limit: 2
fp16: false
